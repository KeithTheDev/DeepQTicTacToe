File: ./tic_tac_toe/__init__.py
File: ./tic_tac_toe/model.py
import torch
import torch.nn as nn


class DQN(nn.Module):
    def __init__(self, architecture='default'):
        super(DQN, self).__init__()
        if architecture == 'default':
            self.fc1 = nn.Linear(9, 128)
            self.fc2 = nn.Linear(128, 128)
            self.value_stream = nn.Linear(128, 1)
            self.advantage_stream = nn.Linear(128, 9)
        elif architecture == 'small':
            self.fc1 = nn.Linear(9, 64)
            self.fc2 = nn.Linear(64, 64)
            self.value_stream = nn.Linear(64, 1)
            self.advantage_stream = nn.Linear(64, 9)
        elif architecture == 'large':
            self.fc1 = nn.Linear(9, 256)
            self.fc2 = nn.Linear(256, 256)
            self.value_stream = nn.Linear(256, 1)
            self.advantage_stream = nn.Linear(256, 9)
        else:
            raise ValueError("Unknown architecture type")

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        value = self.value_stream(x)
        advantage = self.advantage_stream(x)
        return value + (advantage - advantage.mean())
File: ./tic_tac_toe/train.py
import json
import os
import random

import numpy as np
import optuna
import pandas as pd
import torch
import torch.nn.functional as F
import torch.optim as optim
from tic_tac_toe.environment import TicTacToe
from tic_tac_toe.model import DQN
from utils.replay_buffer import PrioritizedReplayBuffer

CHECKPOINT_DIR = './checkpoints'
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

def choose_action(state, policy_net, epsilon, env):
    if random.random() < epsilon:
        return random.choice(env.available_actions())
    else:
        with torch.no_grad():
            state = torch.tensor(state.flatten(), dtype=torch.float32).unsqueeze(0)
            q_values = policy_net(state)
            available_actions = env.available_actions()
            available_q_values = [q_values[0, action[0] * 3 + action[1]].item() for action in available_actions]
            max_q_value = max(available_q_values)
            max_q_action = available_actions[available_q_values.index(max_q_value)]
            return max_q_action

def optimize_model(memory, policy_net, target_net, optimizer, batch_size, gamma, beta):
    if len(memory) < batch_size:
        return 0
    states, actions, rewards, next_states, dones, indices, weights = memory.sample(batch_size, beta)

    states = torch.tensor(states, dtype=torch.float32)
    actions = torch.tensor(actions, dtype=torch.long)
    rewards = torch.tensor(rewards, dtype=torch.float32)
    next_states = torch.tensor(next_states, dtype=torch.float32)
    dones = torch.tensor(dones, dtype=torch.float32)
    weights = torch.tensor(weights, dtype=torch.float32)

    state_action_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)
    next_state_actions = policy_net(next_states).max(1)[1].unsqueeze(1)
    next_state_values = target_net(next_states).gather(1, next_state_actions).squeeze(1).detach()
    expected_state_action_values = rewards + (gamma * next_state_values * (1 - dones))

    loss = (state_action_values - expected_state_action_values).pow(2) * weights
    prios = loss + 1e-5
    loss = loss.mean()

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    memory.update_priorities(indices, prios.detach().cpu().numpy())
    
    return loss.item()

def save_checkpoint(policy_net, target_net, optimizer, episode, architecture):
    checkpoint_path = os.path.join(CHECKPOINT_DIR, f'checkpoint_{episode}.pth')
    torch.save({
        'policy_net_state_dict': policy_net.state_dict(),
        'target_net_state_dict': target_net.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'episode': episode,
        'architecture': architecture
    }, checkpoint_path)
    print(f"Checkpoint saved at {checkpoint_path}")

def load_checkpoint(policy_net, target_net, architecture, optimizer=None):
    checkpoints = [f for f in os.listdir(CHECKPOINT_DIR) if f.startswith('checkpoint_')]
    if not checkpoints:
        return 0
    latest_checkpoint = max(checkpoints, key=lambda f: int(f.split('_')[1].split('.')[0]))
    checkpoint_path = os.path.join(CHECKPOINT_DIR, latest_checkpoint)
    checkpoint = torch.load(checkpoint_path)
    
    if checkpoint.get('architecture', 'default') != architecture:
        print("Skipping checkpoint loading due to architecture mismatch.")
        return 0
    
    policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
    target_net.load_state_dict(checkpoint['target_net_state_dict'])
    if optimizer is not None:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    episode = checkpoint['episode']
    print(f"Checkpoint loaded from {checkpoint_path}, resuming from episode {episode}")
    return episode + 1

def reinitialize_weights(model):
    for layer in model.children():
        if hasattr(layer, 'reset_parameters'):
            layer.reset_parameters()

def train_dqn(env, policy_net, target_net, num_episodes=5000, batch_size=128, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=500, target_update=50, save_interval=1000, architecture='default', update_checkpoints=True, alpha=0.6, beta_start=0.4):
    optimizer = optim.Adam(policy_net.parameters())
    memory = PrioritizedReplayBuffer(10000, alpha)

    start_episode = load_checkpoint(policy_net, target_net, architecture, optimizer) if update_checkpoints else 0

    total_loss = 0

    for episode in range(start_episode, num_episodes):
        state = env.reset()
        episode_loss = 0
        epsilon = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-1. * episode / epsilon_decay)
        beta = min(1.0, beta_start + episode * (1.0 - beta_start) / num_episodes)
        for t in range(9):
            action = choose_action(state, policy_net, epsilon, env)
            while True:
                try:
                    next_state, done, winner = env.step(action, 1 if t % 2 == 0 else -1)
                    break
                except ValueError:
                    action = choose_action(state, policy_net, epsilon, env)
            reward = 1 if winner == 1 else -1 if winner == -1 else 0
            memory.push(state.flatten(), action[0] * 3 + action[1], reward, next_state.flatten(), done)
            state = next_state
            if done:
                break
            loss = optimize_model(memory, policy_net, target_net, optimizer, batch_size, gamma, beta)
            episode_loss += loss

        if episode % target_update == 0:
            target_net.load_state_dict(policy_net.state_dict())

        if episode % save_interval == 0 and update_checkpoints:
            save_checkpoint(policy_net, target_net, optimizer, episode, architecture)

        if episode % 100 == 0:
            avg_loss = episode_loss / (t + 1)
            print(f"Episode {episode}/{num_episodes}, Loss: {avg_loss:.6f}")

        total_loss += episode_loss

    avg_total_loss = total_loss / num_episodes
    print("Training complete.")
    return avg_total_loss

def objective(trial):
    env = TicTacToe()
    policy_net = DQN(architecture='default')
    target_net = DQN(architecture='default')
    target_net.load_state_dict(policy_net.state_dict())
    reinitialize_weights(policy_net)
    reinitialize_weights(target_net)
    
    num_episodes = trial.suggest_int('num_episodes', 1000, 5000)
    batch_size = trial.suggest_int('batch_size', 32, 128)
    gamma = trial.suggest_uniform('gamma', 0.9, 0.99)
    epsilon_start = trial.suggest_uniform('epsilon_start', 0.8, 1.0)
    epsilon_end = trial.suggest_uniform('epsilon_end', 0.01, 0.1)
    epsilon_decay = trial.suggest_int('epsilon_decay', 100, 1000)
    target_update = trial.suggest_int('target_update', 10, 50)
    save_interval = trial.suggest_int('save_interval', 1000, 5000)

    avg_loss = train_dqn(
        env, policy_net, target_net,
        num_episodes=num_episodes,
        batch_size=batch_size,
        gamma=gamma,
        epsilon_start=epsilon_start,
        epsilon_end=epsilon_end,
        epsilon_decay=epsilon_decay,
        target_update=target_update,
        save_interval=save_interval,
        update_checkpoints=False
    )
    
    return avg_loss

def run_experiments(num_experiments=10, num_episodes=5000):
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=num_experiments, n_jobs=4)  # Using 4 parallel jobs

    best_trial = study.best_trial
    print(f"Best hyperparameters: {best_trial.params} with avg loss: {best_trial.value:.6f}")

    # Convert DataFrame to dict with stringified timestamps and timedeltas
    trials_df = study.trials_dataframe().applymap(
        lambda x: str(x) if isinstance(x, (np.datetime64, pd.Timestamp, pd.Timedelta)) else x
    ).to_dict()
    with open('experiment_results_fine_tuned.json', 'w') as f:
        json.dump(trials_df, f, indent=4)
File: ./tic_tac_toe/environment.py
import numpy as np


class TicTacToe:
    def __init__(self):
        self.board = np.zeros((3, 3), dtype=int)
        self.done = False
        self.winner = None

    def reset(self):
        self.board = np.zeros((3, 3), dtype=int)
        self.done = False
        self.winner = None
        return self.board

    def available_actions(self):
        return [(i, j) for i in range(3) for j in range(3) if self.board[i, j] == 0]

    def step(self, action, player):
        if self.board[action] != 0:
            raise ValueError("Invalid move")
        self.board[action] = player
        if self.check_winner(player):
            self.done = True
            self.winner = player
        elif not self.available_actions():
            self.done = True
            self.winner = 0
        return self.board, self.done, self.winner

    def check_winner(self, player):
        for i in range(3):
            if all([self.board[i, j] == player for j in range(3)]) or all([self.board[j, i] == player for j in range(3)]):
                return True
        if all([self.board[i, i] == player for i in range(3)]) or all([self.board[i, 2 - i] == player for i in range(3)]):
            return True
        return False

    def render(self):
        for row in self.board:
            print(" | ".join(["X" if x == 1 else "O" if x == -1 else " " for x in row]))
            print("---------")
File: ./utils/__init__.py
File: ./utils/replay_buffer.py
import random

import numpy as np


class PrioritizedReplayBuffer:
    def __init__(self, capacity, alpha=0.6):
        self.capacity = capacity
        self.memory = []
        self.priorities = []
        self.position = 0
        self.alpha = alpha

    def push(self, *args):
        max_priority = max(self.priorities, default=1.0)
        if len(self.memory) < self.capacity:
            self.memory.append(None)
            self.priorities.append(None)
        self.memory[self.position] = args
        self.priorities[self.position] = max_priority
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size, beta=0.4):
        if len(self.memory) == self.capacity:
            priorities = np.array(self.priorities)
        else:
            priorities = np.array(self.priorities[:self.position])
        priorities = priorities ** self.alpha
        probabilities = priorities / priorities.sum()

        indices = np.random.choice(len(self.memory), batch_size, p=probabilities)
        samples = [self.memory[i] for i in indices]

        total = len(self.memory)
        weights = (total * probabilities[indices]) ** (-beta)
        weights /= weights.max()
        weights = np.array(weights, dtype=np.float32)

        batch = list(zip(*samples))
        states = np.array(batch[0])
        actions = np.array(batch[1])
        rewards = np.array(batch[2])
        next_states = np.array(batch[3])
        dones = np.array(batch[4])

        return states, actions, rewards, next_states, dones, indices, weights

    def update_priorities(self, batch_indices, batch_priorities):
        for idx, priority in zip(batch_indices, batch_priorities):
            self.priorities[idx] = priority

    def __len__(self):
        return len(self.memory)
File: ./tic_tac_toe_gameplay.py
import random
import sys

import pygame
from tic_tac_toe.environment import TicTacToe
from tic_tac_toe.train import choose_action


def run_pygame(policy_net, target_net):
    pygame.init()

    WIDTH, HEIGHT = 300, 300
    LINE_WIDTH = 5
    BOARD_ROWS, BOARD_COLS = 3, 3
    SQUARE_SIZE = WIDTH // BOARD_COLS
    CIRCLE_RADIUS = SQUARE_SIZE // 3
    CIRCLE_WIDTH = 15
    CROSS_WIDTH = 25
    SPACE = SQUARE_SIZE // 4

    BG_COLOR = (28, 170, 156)
    LINE_COLOR = (23, 145, 135)
    CIRCLE_COLOR = (239, 231, 200)
    CROSS_COLOR = (84, 84, 84)

    screen = pygame.display.set_mode((WIDTH, HEIGHT))
    pygame.display.set_caption('Tic Tac Toe')

    font = pygame.font.Font(None, 100)

    env = TicTacToe()

    ai_wins = 0
    human_wins = 0
    ties = 0

    def draw_lines():
        screen.fill(BG_COLOR)
        for row in range(1, BOARD_ROWS):
            pygame.draw.line(screen, LINE_COLOR, (0, row * SQUARE_SIZE), (WIDTH, row * SQUARE_SIZE), LINE_WIDTH)
            pygame.draw.line(screen, LINE_COLOR, (row * SQUARE_SIZE, 0), (row * SQUARE_SIZE, HEIGHT), LINE_WIDTH)

    def draw_figures():
        for row in range(BOARD_ROWS):
            for col in range(BOARD_COLS):
                if env.board[row][col] == 1:
                    pygame.draw.line(screen, CROSS_COLOR, (col * SQUARE_SIZE + SPACE, row * SQUARE_SIZE + SPACE), 
                                     (col * SQUARE_SIZE + SQUARE_SIZE - SPACE, row * SQUARE_SIZE + SQUARE_SIZE - SPACE), CROSS_WIDTH)
                    pygame.draw.line(screen, CROSS_COLOR, (col * SQUARE_SIZE + SPACE, row * SQUARE_SIZE + SQUARE_SIZE - SPACE), 
                                     (col * SQUARE_SIZE + SQUARE_SIZE - SPACE, row * SQUARE_SIZE + SPACE), CROSS_WIDTH)
                elif env.board[row][col] == -1:
                    pygame.draw.circle(screen, CIRCLE_COLOR, 
                                       (int(col * SQUARE_SIZE + SQUARE_SIZE // 2), int(row * SQUARE_SIZE + SQUARE_SIZE // 2)), CIRCLE_RADIUS, CIRCLE_WIDTH)

    def check_winner(player):
        for row in range(BOARD_ROWS):
            if all([env.board[row][col] == player for col in range(BOARD_COLS)]):
                return True
        for col in range(BOARD_COLS):
            if all([env.board[row][col] == player for row in range(BOARD_ROWS)]):
                return True
        if all([env.board[i][i] == player for i in range(BOARD_ROWS)]) or all([env.board[i][BOARD_ROWS - i - 1] == player for i in range(BOARD_ROWS)]):
            return True
        return False

    def draw_winner(player):
        nonlocal ai_wins, human_wins, ties
        if player == 1:
            text = 'X wins!'
            human_wins += 1
        elif player == -1:
            text = 'O wins!'
            ai_wins += 1
        else:
            text = 'It\'s a tie!'
            ties += 1
        label = font.render(text, True, CIRCLE_COLOR if player == -1 else CROSS_COLOR)
        screen.blit(label, (WIDTH // 2 - label.get_width() // 2, HEIGHT // 2 - label.get_height() // 2))
        pygame.display.update()
        pygame.time.delay(2000)

    def draw_score():
        score_text = f"AI: {ai_wins} Human: {human_wins} Ties: {ties}"
        label = font.render(score_text, True, (255, 255, 255))
        screen.blit(label, (10, 10))

    def reset_game():
        draw_score()
        env.reset()
        draw_lines()
        draw_figures()
        new_player = -1 if random.random() < 0.5 else 1
        print(f"New game starting. Player {'O' if new_player == -1 else 'X'} goes first.")
        return new_player

    draw_lines()
    player = 1
    game_over = False

    while True:
        if player == -1 and not game_over:
            if env.available_actions():
                state = env.board
                action = choose_action(state, policy_net, 0, env)
                env.step(action, player)
                if check_winner(player):
                    game_over = True
                    draw_winner(player)
                    player = reset_game()
                    game_over = False
                else:
                    player *= -1
                draw_figures()
            else:
                game_over = True
                draw_winner(0)
                player = reset_game()
                game_over = False

        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                pygame.quit()
                sys.exit()
            if event.type == pygame.MOUSEBUTTONDOWN and not game_over:
                mouseX = event.pos[0]
                mouseY = event.pos[1]
                clicked_row = mouseY // SQUARE_SIZE
                clicked_col = mouseX // SQUARE_SIZE

                if env.board[clicked_row][clicked_col] == 0:
                    env.board[clicked_row][clicked_col] = player
                    if check_winner(player):
                        game_over = True
                        draw_winner(player)
                        player = reset_game()
                        game_over = False
                    else:
                        player *= -1
                    draw_figures()

        pygame.display.update()
File: ./main.py
import argparse
import random
import sys

import pygame
from tic_tac_toe.environment import TicTacToe
from tic_tac_toe.model import DQN
from tic_tac_toe.train import (choose_action, load_checkpoint, run_experiments,
                               train_dqn)
from tic_tac_toe_gameplay import run_pygame

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train or play Tic-Tac-Toe with DQN")
    parser.add_argument('--mode', choices=['train', 'play', 'experiment'], required=True, help="Mode to run: train, play, or experiment")

    args = parser.parse_args()

    env = TicTacToe()
    architecture = 'default'
    policy_net = DQN(architecture=architecture)
    target_net = DQN(architecture=architecture)
    target_net.load_state_dict(policy_net.state_dict())

    if args.mode == 'train':
        best_hyperparameters = {
            'num_episodes': 100000,
            #'num_episodes': 4923,
            'batch_size': 109,
            'gamma': 0.9007905885073516,
            'epsilon_start': 0.8982950447165381,
            'epsilon_end': 0.07793103251263457,
            'epsilon_decay': 416,
            'target_update': 24,
            'save_interval': 2669
        }

        train_dqn(
            env, policy_net, target_net,
            num_episodes=best_hyperparameters['num_episodes'],
            batch_size=best_hyperparameters['batch_size'],
            gamma=best_hyperparameters['gamma'],
            epsilon_start=best_hyperparameters['epsilon_start'],
            epsilon_end=best_hyperparameters['epsilon_end'],
            epsilon_decay=best_hyperparameters['epsilon_decay'],
            target_update=best_hyperparameters['target_update'],
            save_interval=best_hyperparameters['save_interval'],
            architecture=architecture
        )
    elif args.mode == 'experiment':
        run_experiments(num_experiments=10, num_episodes=5000)
    elif args.mode == 'play':
        load_checkpoint(policy_net, target_net, architecture=architecture)
        run_pygame(policy_net, target_net)
